{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programming assignment 4: Implementing linear classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed libraries for all the tasks\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from cleantext import clean\n",
    "from string import digits\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm  import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "X1 = [{'city':'Gothenburg', 'month':'July'},\n",
    "      {'city':'Gothenburg', 'month':'December'},\n",
    "      {'city':'Paris', 'month':'July'},\n",
    "      {'city':'Paris', 'month':'December'}]\n",
    "Y1 = ['rain', 'rain', 'sun', 'rain']\n",
    "\n",
    "X2 = [{'city':'Sydney', 'month':'July'},\n",
    "      {'city':'Sydney', 'month':'December'},\n",
    "      {'city':'Paris', 'month':'July'},\n",
    "      {'city':'Paris', 'month':'December'}]\n",
    "Y2 = ['rain', 'sun', 'sun', 'rain']\n",
    "\n",
    "classifier1 = make_pipeline(DictVectorizer(), Perceptron(max_iter=10))\n",
    "classifier1.fit(X1, Y1)\n",
    "guesses1 = classifier1.predict(X1)\n",
    "print(accuracy_score(Y1, guesses1))\n",
    "\n",
    "classifier2 = make_pipeline(DictVectorizer(), Perceptron(max_iter=10))\n",
    "classifier2.fit(X2, Y2)\n",
    "guesses2 = classifier2.predict(X2)\n",
    "print(accuracy_score(Y2, guesses2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the perceptron algorithm is a linear classifier that finds a hyperplane to separate data into different classes. If there is clear separation between the classes, it can perfectly classify the data,same as X1, that has a clear separation between the two classes of \"rain\" and \"sun\" based on the month and city. On the other hand, in X2 we see overlapping  between the classes, such as Paris in July, that can be classified as both \"sun\" and \"rain\". This overlap means there is no single hyperplane that will perfectly classify all instances. As can be seen below, the accuracy does not imprve by using Linear SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "classifier3 = make_pipeline(DictVectorizer(), LinearSVC())\n",
    "classifier3.fit(X1, Y1)\n",
    "guesses3 = classifier3.predict(X1)\n",
    "print(accuracy_score(Y1, guesses3))\n",
    "\n",
    "classifier4 = make_pipeline(DictVectorizer(), LinearSVC())\n",
    "classifier4.fit(X2, Y2)\n",
    "guesses3 = classifier4.predict(X2)\n",
    "print(accuracy_score(Y2, guesses3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(BaseEstimator):\n",
    "    \"\"\"\n",
    "    General class for binary linear classifiers. Implements the predict\n",
    "    function, which is the same for all binary linear classifiers. There are\n",
    "    also two utility functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Computes the decision function for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "        return X.dot(self.w)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the outputs for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "\n",
    "        # First compute the output scores\n",
    "        scores = self.decision_function(X)\n",
    "\n",
    "        # Select the positive or negative class label, depending on whether\n",
    "        # the score was positive or negative.\n",
    "        out = np.select([scores >= 0.0, scores < 0.0],\n",
    "                        [self.positive_class,\n",
    "                         self.negative_class])\n",
    "        return out\n",
    "\n",
    "    def find_classes(self, Y):\n",
    "        \"\"\"\n",
    "        Finds the set of output classes in the output part Y of the training set.\n",
    "        If there are exactly two classes, one of them is associated to positive\n",
    "        classifier scores, the other one to negative scores. If the number of\n",
    "        classes is not 2, an error is raised.\n",
    "        \"\"\"\n",
    "        classes = sorted(set(Y))\n",
    "        if len(classes) != 2:\n",
    "            raise Exception(\"this does not seem to be a 2-class problem\")\n",
    "        self.positive_class = classes[1]\n",
    "        self.negative_class = classes[0]\n",
    "\n",
    "    def encode_outputs(self, Y):\n",
    "        \"\"\"\n",
    "        A helper function that converts all outputs to +1 or -1.\n",
    "        \"\"\"\n",
    "        return np.array([1 if y == self.positive_class else -1 for y in Y])\n",
    "\n",
    "\n",
    "class Perceptron(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        # Perceptron algorithm:\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in zip(X, Ye):\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                score = x.dot(self.w)\n",
    "\n",
    "                # If there was an error, update the weights.\n",
    "                if y*score <= 0:\n",
    "                    self.w += y*x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for perception is:  0.7918590012589173\n"
     ]
    }
   ],
   "source": [
    "# reading data from corpus file and spliting each line into\n",
    "# its label and text, and appends them to the Y and X lists\n",
    "def read_data(corpus_file):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            _, y, _, x = line.split(maxsplit=3)\n",
    "            X.append(x.strip())\n",
    "            Y.append(y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Read all the documents.\n",
    "    X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "    \n",
    "    # Split into training and test parts.\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, random_state=0)\n",
    "\n",
    "    # preprocessing steps\n",
    "    # 1st: converts the text into a sparse matrix of TF-IDF features\n",
    "    # 2nd: elects the top 1000 features based on their score\n",
    "    # 3rd: normalizes the feature vectors to unit length\n",
    "    # 4th: Using Perception as the Linear Classifier\n",
    "    pipeline = make_pipeline(\n",
    "        TfidfVectorizer(),\n",
    "        SelectKBest(k=1000),\n",
    "        Normalizer(),\n",
    "        Perceptron()  \n",
    "    )\n",
    "\n",
    "    # Train the classifier\n",
    "    pipeline.fit(X_train, Y_train)\n",
    "\n",
    "    # Evaluate on the test set.\n",
    "    Y_guess = pipeline.predict(X_test)\n",
    "    Accuracy = accuracy_score(Y_test, Y_guess)\n",
    "    print(f\"Accuracy for perception is:  {Accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVC(LinearClassifier):\n",
    "    \n",
    "    # A straightforward implementation of the perceptron learning algorithm.\n",
    "\n",
    "    def __init__(self, reg_lambda = 0.001, n_iter=20):\n",
    "        # n_iter specifies the number of iterations through the training set,chosen by constructor\n",
    "\n",
    "        self.n_iter = n_iter\n",
    "        self.reg_lambda = reg_lambda\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the SVC learning algorithm.\n",
    "        \"\"\"\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            t = 0\n",
    "            for x, y in zip(X, Ye):\n",
    "                t = t + 1\n",
    "                eta = 1 / (self.reg_lambda * t)\n",
    "                score = np.dot(x, self.w)\n",
    "                self.w = (1 - eta * self.reg_lambda) * self.w\n",
    "                # if misclassification: w is updated using the perceptron update rule\n",
    "                if score*y < 1:\n",
    "                    self.w += np.dot(x , eta * y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Linear SCV is:  %81.91355434326479\n",
      "Training time for Linear SCV: 7.603312969207764\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "        TfidfVectorizer(),\n",
    "        SelectKBest(k=1000),\n",
    "        Normalizer(),\n",
    "        SVC()\n",
    "    )\n",
    "\n",
    "# Training the classifier an calculating the training time\n",
    "t0 = time.time()\n",
    "pipeline.fit(X_train, Y_train)\n",
    "t1 = time.time()\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Y_guess = pipeline.predict(X_test)\n",
    "Accuracy = accuracy_score(Y_test, Y_guess)\n",
    "\n",
    "print(f\"Accuracy for Linear SCV is:  % {Accuracy * 100}\")\n",
    "print(f\"Training time for Linear SCV: {t1 - t0}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(LinearClassifier):\n",
    "    \n",
    "    def __init__(self, reg_lambda = 0.0001, n_iter=30):\n",
    "        # n_iter specifies the number of iterations through the training set,chosen by constructor\n",
    "\n",
    "        self.n_iter = n_iter\n",
    "        self.reg_lambda = reg_lambda\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the logistic regression learning algorithm.\n",
    "        \"\"\"\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            t = 0\n",
    "            for x, y in zip(X, Ye):\n",
    "                t = t + 1\n",
    "                eta = 1 / (self.reg_lambda * t)\n",
    "                score = np.dot(x, self.w)\n",
    "                self.w = (1 - eta * self.reg_lambda) * self.w + y / (1 + np.exp(y * self.w * x)) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Logistic Regression is: % 83.71800251783466\n",
      "Training time for Logistic Regression: 2.445495128631592\n"
     ]
    }
   ],
   "source": [
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "        TfidfVectorizer(),\n",
    "        SelectKBest(k=1000),\n",
    "        Normalizer(),\n",
    "        LogisticRegression()\n",
    "    )\n",
    "\n",
    "# Training the classifier and calculating training time\n",
    "t0 = time.time()\n",
    "pipeline.fit(X_train, Y_train)\n",
    "t1 = time.time()\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Y_guess = pipeline.predict(X_test)\n",
    "Accuracy = accuracy_score(Y_test, Y_guess)\n",
    "\n",
    "print(f\"Accuracy for Logistic Regression is: % {Accuracy * 100}\")\n",
    "print(f\"Training time for Logistic Regression: {t1 - t0}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7230b5a7051959ff11a2accd3c4fdc58a26a68b63b2b2515e318deebc45e0686"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
